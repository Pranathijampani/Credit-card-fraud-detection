import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (confusion_matrix, classification_report,
                             roc_auc_score, precision_recall_curve, auc,
                             average_precision_score)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance

from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

import joblib
python
Copy code
# 2. Load data
df = pd.read_csv('data/creditcard.csv')  # adjust path
df.shape, df['Class'].value_counts()
python
Copy code
# 3. Quick EDA / class imbalance
print(df['Class'].value_counts(normalize=True))
sns.countplot(x='Class', data=df)
plt.title('Class distribution (0 = legit, 1 = fraud)')
plt.show()

# Amount & Time distributions
fig, axes = plt.subplots(1,2, figsize=(12,4))
sns.histplot(df['Amount'], bins=50, ax=axes[0])
axes[0].set_title('Transaction Amount')
sns.histplot(df['Time'], bins=50, ax=axes[1])
axes[1].set_title('Transaction Time')
plt.show()
python
Copy code
# 4. Preprocessing: features and split
X = df.drop('Class', axis=1)
y = df['Class']

# Scale Amount and Time (other features are PCA-ed)
scaler = StandardScaler()
X[['Time','Amount']] = scaler.fit_transform(X[['Time','Amount']])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)
print(X_train.shape, X_test.shape, y_train.mean(), y_test.mean())
python
Copy code
# 5. Baseline: Logistic Regression with class_weight balanced
lr = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)[:,1]

print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_proba))
print("Average precision (PR AUC):", average_precision_score(y_test, y_proba))
python
Copy code
# 6. Better pipeline using SMOTE + RandomForest
sm = SMOTE(random_state=42)
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)

pipeline = ImbPipeline(steps=[('smote', sm), ('rf', rf)])
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)[:,1]

print(classification_report(y_test, y_pred))
print("ROC AUC:", roc_auc_score(y_test, y_proba))
print("Average precision:", average_precision_score(y_test, y_proba))
python
Copy code
# 7. Precision-Recall curve (preferred for imbalanced)
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall, precision)
print("PR AUC:", pr_auc)

plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (PR AUC = {:.4f})'.format(pr_auc))
plt.show()
python
Copy code
# 8. Feature importance (permutation)
r = permutation_importance(pipeline.named_steps['rf'], X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
sorted_idx = r.importances_mean.argsort()[::-1]
for i in sorted_idx[:10]:
    print(X.columns[i], r.importances_mean[i])
python
Copy code
# 9. Save model + scaler
joblib.dump(pipeline, 'models/fraud_rf_smote.joblib')
joblib.dump(scaler, 'models/scaler.joblib')
Optional: XGBoost / LightGBM + GridSearch (hyperparameter tuning)
python
Copy code
from xgboost import XGBClassifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)

imb_pipeline = ImbPipeline([('smote', SMOTE(random_state=42)), ('xgb', xgb)])
param_grid = {
    'xgb__n_estimators': [100, 200],
    'xgb__max_depth': [3, 6],
    'xgb__learning_rate': [0.1, 0.01],
}
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
gs = GridSearchCV(imb_pipeline, param_grid, scoring='average_precision', cv=cv, n_jobs=-1)
gs.fit(X_train, y_train)
print("Best params:", gs.best_params_)
best = gs.best_estimator_
y_proba = best.predict_proba(X_test)[:,1]
print("PR AUC:", average_precision_score(y_test, y_proba))
Explainability (SHAP)
python
Copy code
# Use SHAP for model explainability if available
import shap
explainer = shap.TreeExplainer(pipeline.named_steps['rf'])
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)  # class 1 explanations
Deployment (Streamlit minimal example)
python
Copy code
# app/streamlit_app.py
import streamlit as st
import pandas as pd
import joblib

model = joblib.load('models/fraud_rf_smote.joblib')
scaler = joblib.load('models/scaler.joblib')

st.title("Credit Card Fraud Detection Demo")
uploaded = st.file_uploader("Upload transactions CSV (same format)", type=['csv'])
if uploaded:
    data = pd.read_csv(uploaded)
    data[['Time','Amount']] = scaler.transform(data[['Time','Amount']])
    proba = model.predict_proba(data)[:,1]
    data['fraud_prob'] = proba
    st.dataframe(data.head(50))
    st.write("Top likely frauds")
    st.dataframe(data.sort_values('fraud_prob', ascending=False).head(20))
